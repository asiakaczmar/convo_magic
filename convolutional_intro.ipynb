{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import downsample\n",
    "\n",
    "theano.config.exception_verbosity='high'\n",
    "rng = numpy.random.RandomState(23455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_convolutionaln_function(filters_number, filter_width, filter_height, input_depth):\n",
    "    input_ = T.tensor4(name='input')\n",
    "    w_shp = (filters_number, input_depth, filter_width, filter_height)\n",
    "    #if weights are boud\n",
    "    w_bound = numpy.sqrt(input_depth * filter_width * filter_height)\n",
    "    W = theano.shared( numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-1.0 / w_bound,\n",
    "                    high=1.0 / w_bound,\n",
    "                    size=w_shp),\n",
    "                dtype=input.dtype), name ='W')\n",
    "    b_shp = (2,)\n",
    "    b = theano.shared(numpy.asarray(\n",
    "                rng.uniform(low=-.5, high=.5, size=b_shp),\n",
    "                dtype=input.dtype), name ='b')\n",
    "    # build symbolic expression that computes the convolution of input with filters in w\n",
    "    conv_out = conv2d(input_, W)\n",
    "    return T.nnet.sigmoid(conv_out + b.dimshuffle('x', 0, 'x', 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shared_dataset(data_xy, borrow=True):\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                           dtype=theano.config.floatX),\n",
    "                             borrow=borrow)\n",
    "    shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                           dtype=theano.config.floatX),\n",
    "                             borrow=borrow)\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "            \n",
    "\n",
    "def load_data(path='/home/siak/Data/MNIST/mnist.pkl.gz'):\n",
    "    # Load the dataset\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "\n",
    "   \n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvLayer(object):\n",
    "    \"\"\"Pool Layer of a convolutional network \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input_, filter_shape, input_shape):\n",
    "        \"\"\"\n",
    "        :filter_shape: (number of output feature maps, num input feature maps,\n",
    "                              filter height, filter width)\n",
    "\n",
    "        :input_shape: (batch size, num input feature maps,\n",
    "                             image height, image width)\n",
    "        \"\"\"\n",
    "\n",
    "        assert input_shape[1] == filter_shape[1]\n",
    "        self.input_ = input_\n",
    "        number_of_inputs = numpy.prod(filter_shape[1:])\n",
    "        number_of_outputs = numpy.prod(filter_shape[0] + numpy.prod(filter_shape[2:]))\n",
    "\n",
    "        self.W = theano.shared(\n",
    "            numpy.asarray(rng.uniform(low=-1, high=1, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # the bias is a 1D tensor -- one bias per output feature map\n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        # convolve input feature maps with filters\n",
    "        conv_out = conv2d(\n",
    "            input=input_,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=input_shape\n",
    "        )\n",
    "\n",
    "\n",
    "        # add the bias term. Since the bias is a vector (1D array), we first\n",
    "        # reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n",
    "        # thus be broadcasted across mini-batches and feature map\n",
    "        # width & height\n",
    "        self.output = conv_out + self.b.dimshuffle('x', 0, 'x', 'x')\n",
    "\n",
    "        # store parameters of this layer\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input_ = input_\n",
    "        \n",
    "        \n",
    "#remember to add zero padding\n",
    "class PoolLayer(object):\n",
    "    def __init__(self, rng, input_, poolsize=(2, 2)):\n",
    "        self.output = downsample.max_pool_2d(\n",
    "            input=input_,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "        self.input_ = input_\n",
    "        \n",
    "class ReluLayer(object):\n",
    "    def __init__(self, input_):\n",
    "        self.output = T.tanh(input_)\n",
    "        self.input_ = input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvoNet(object):\n",
    "    def initialize_datasets(self):\n",
    "        datasets = load_data()\n",
    "        self.train_set_x, self.train_set_y = datasets[0]\n",
    "        self.valid_set_x, self.valid_set_y = datasets[1]\n",
    "        self.test_set_x, self.test_set_y = datasets[2]\n",
    "\n",
    "        self.n_train_batches = self.train_set_x.get_value(borrow=True).shape[0]\n",
    "        self.n_valid_batches = self.valid_set_x.get_value(borrow=True).shape[0]\n",
    "        self.n_test_batches = self.test_set_x.get_value(borrow=True).shape[0]\n",
    "        self.n_train_batches //= self.batch_size\n",
    "        self.n_valid_batches //= self.batch_size\n",
    "        self.n_test_batches //= self.batch_size\n",
    "    \n",
    "    def setup_network(self):\n",
    "        input_shape = (self.batch_size, 1, 28, 28)\n",
    "        layer0_input = self.x.reshape(input_shape)\n",
    "        self.layer0 = ConvLayer(self.state, input_=layer0_input, filter_shape=(16, 1, 5, 5), input_shape=input_shape)\n",
    "        \n",
    "        self.layer1 = ReluLayer(input_=self.layer0.output)\n",
    "        self.input_data = self.test_set_x[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "        \n",
    "       \n",
    "        self.layer2 = ConvLayer(self.state, input_= self.layer1.output, filter_shape=(32, 16, 5, 5), input_shape=(self.batch_size, 16, 24, 24))\n",
    "        self.layer3 = ReluLayer(input_=self.layer2.output)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        self.layer4 = PoolLayer(self.state, input_=self.layer3.output)\n",
    "        \n",
    "        self.layer5 = HiddenLayer(self.state, input_=self.layer4.output.flatten(2), n_in= 32 * 10 * 10,\n",
    "        n_out=500, activation=T.tanh)\n",
    "        self.layer6 =  LogisticRegression(input_= self.layer5.output, n_in=500, n_out=10)\n",
    "    \n",
    "    def setup_training_functions(self):\n",
    "        self.validate_model = theano.function(\n",
    "            [self.index],\n",
    "            self.layer6.errors(self.y),\n",
    "            givens={\n",
    "                self.x: self.test_set_x[self.index * self.batch_size: (self.index + 1) * self.batch_size],\n",
    "                self.y: self.test_set_y[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "        })\n",
    "        self.cost = self.layer6.negative_log_likelihood(self.y)\n",
    "        self.params = self.layer6.params + self.layer5.params + self.layer2.params + self.layer0.params\n",
    "        self.grads = T.grad(self.cost, self.params)\n",
    "        self.updates = [(param_i, param_i - self.learning_rate * grad_i)\n",
    "                   for param_i, grad_i in zip(self.params, self.grads)\n",
    "                  ]\n",
    "        self.train_model = theano.function(\n",
    "            [self.index],\n",
    "            self.cost,\n",
    "            updates=self.updates,\n",
    "            givens={\n",
    "                self.x: self.train_set_x[self.index * self.batch_size: (self.index + 1) * self.batch_size],\n",
    "                self.y: self.train_set_y[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            }\n",
    "        )\n",
    "    def train(self):\n",
    "        for epoch in xrange(self.epochs_no):\n",
    "            for minibatch_index in range(self.n_train_batches):\n",
    "                cost_ij = self.train_model(minibatch_index)\n",
    "                validation_losses = [self.validate_model(i) for i\n",
    "                                     in range(self.n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, self.n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "        print('optimization complete!')\n",
    "    \n",
    "    def __init__(self, batch_size=500, learning_rate=0.1, epochs_no=200):\n",
    "        self.state = numpy.random.RandomState(23455)\n",
    "        self.index = T.lscalar()  # index to a [mini]batch\n",
    "        self.batch_size = batch_size\n",
    "        self.x = T.matrix('x')   # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs_no = epochs_no\n",
    "        self.initialize_datasets()\n",
    "        self.setup_network()\n",
    "        self.setup_training_functions()\n",
    "        \n",
    "\n",
    "        \n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cn = ConvoNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, minibatch 1/100, validation error 9.060000 %\n"
     ]
    }
   ],
   "source": [
    "out = cn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.916)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out\n",
    "#for x in range(10):\n",
    "#    print(out[0][0][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "out = cn.test_model(2)\n",
    "im = Image.fromarray(out[0][0])\n",
    "\n",
    "for idx, (x, y) in zip(range(16), window(range(0, 32))):\n",
    "    im = Image.fromarray(out[0][idx] * 256).resize((48, 48), PIL.Image.ANTIALIAS)\n",
    "    fltr = Image.fromarray(cn.layer0.W.get_value()[idx][0] * 256)\n",
    "    fltr.resize((50, 50), PIL.Image.ANTIALIAS)\n",
    "    plt.subplot(16, 2, x+1); plt.axis('off'); plt.imshow(fltr)\n",
    "    plt.subplot(16, 2, y+1); plt.axis('off'); plt.imshow(im)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input_, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This tutorial introduces the multilayer perceptron using Theano.\n",
    "\n",
    " A multilayer perceptron is a logistic regressor where\n",
    "instead of feeding the input to the logistic regression you insert a\n",
    "intermediate layer, called the hidden layer, that has a nonlinear\n",
    "activation function (usually tanh or sigmoid) . One can use many such\n",
    "hidden layers making the architecture deep. The tutorial will also tackle\n",
    "the problem of MNIST digit classification.\n",
    "\n",
    ".. math::\n",
    "\n",
    "    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "# start-snippet-1\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input_, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.dmatrix\n",
    "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input_ = input_\n",
    "        # end-snippet-1\n",
    "\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input_, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window(l):\n",
    "    it = iter(l)\n",
    "    for x in it:\n",
    "        yield (x, next(it))\n",
    "\n",
    "def special_get_value(x):\n",
    "    y = T.cast(x, 'int32')\n",
    "    return y.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
